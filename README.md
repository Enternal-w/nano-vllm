<p align="center">
<img width="300" src="assets/logo.png">
</p>

<p align="center">
<a href="https://trendshift.io/repositories/15323" target="_blank"><img src="https://trendshift.io/api/badge/repositories/15323" alt="GeeeekExplorer%2Fnano-vllm | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</p>

# Nano-vLLM

A lightweight vLLM implementation built from scratch.

## é‡ç‚¹å…³æ³¨
A. å†…å­˜ç®¡ç†æ¨¡å— (BlockManager)
è¿™æ˜¯çµé­‚æ‰€åœ¨ã€‚å…³æ³¨å®ƒæ˜¯å¦‚ä½•è®¡ç®— GPU è¿˜èƒ½å®¹çº³å¤šå°‘ä¸ª Block çš„ã€‚

é‡ç‚¹å…³æ³¨ï¼šå¦‚ä½•ç»´æŠ¤ free_blocks åˆ—è¡¨ï¼Œä»¥åŠåœ¨ forward ä¹‹å‰å¦‚ä½•æ ¹æ®è¾“å…¥é•¿åº¦åˆ†é… block_tableã€‚

B. ç®—å­å®ç°å±‚ (nanovllm/layers/attention.py)
è¿™æ˜¯æœ€ç¡¬æ ¸çš„éƒ¨åˆ†ã€‚é€šå¸¸ä¼šè°ƒç”¨è‡ªå®šä¹‰çš„ CUDA Kernelï¼ˆæˆ–è€… Triton å®ç°ï¼‰ã€‚

é‡ç‚¹å…³æ³¨ï¼šAttention ç±»æ˜¯å¦‚ä½•æ¥æ”¶ block_table çš„ã€‚ä½ ä¼šå‘ç°å®ƒä¸å†æ˜¯ç®€å•çš„ torch.nn.MultiheadAttentionï¼Œè€Œæ˜¯éœ€è¦å¤„ç†ä¸è¿ç»­å†…å­˜çš„ç‰¹æ®Šé€»è¾‘ã€‚

C. è°ƒåº¦é€»è¾‘ (Scheduler)
å†³å®šå“ªäº›è¯·æ±‚ï¼ˆPromptsï¼‰å¯ä»¥è¿›å…¥å½“å‰çš„ Batchã€‚

é‡ç‚¹å…³æ³¨ï¼šå½“æ˜¾å­˜ä¸è¶³ä»¥å®¹çº³ä¸‹ä¸€ä¸ªç”Ÿæˆçš„ Token æ—¶ï¼Œå®ƒæ˜¯å¦‚ä½•å¤„ç†çš„ï¼ˆæ¯”å¦‚æŠ¢å  Preemptionï¼šæš‚åœæŸäº›ä»»åŠ¡å¹¶é‡Šæ”¾å…¶ Blockï¼‰ã€‚

D. æ¨¡å‹å¹¶è¡Œä¸å®šä¹‰ (nanovllm/layers/linear.py)
ä»£ç ä¸­ï¼ŒColumnParallelLinear å’Œ RowParallelLinear æ˜¯åˆ†å¸ƒå¼æ¨ç†çš„åŸºç¡€ã€‚

é‡ç‚¹å…³æ³¨ï¼šAll-Reduce æ“ä½œå‘ç”Ÿåœ¨å“ªé‡Œã€‚

## å­¦ä¹ æ€»ç»“

* BlockManagerå®ç°äº†é€»è¾‘å†…å­˜ä¸ç‰©ç†æ˜¾å­˜çš„è§£è€¦ï¼ŒæŠŠæ˜¾å­˜åˆ‡æˆBlocksï¼ŒæŒ‰éœ€åˆ†é…ã€‚å®ƒè¿˜å¼•å…¥äº† Prefix Cachingï¼ˆå‰ç¼€ç¼“å­˜ï¼‰ã€‚å¦‚æœä¸¤ä¸ªè¯·æ±‚çš„å¼€å¤´æ˜¯ä¸€æ ·çš„ï¼ˆæ¯”å¦‚ç›¸åŒçš„ç³»ç»Ÿæç¤ºè¯ï¼‰ï¼Œå®ƒä»¬ä¼šå…±äº«åŒä¸€ç‰©ç†å—çš„å¼•ç”¨ï¼Œä¸ä»…çœæ˜¾å­˜ï¼Œè¿˜çœå»äº†é‡å¤è®¡ç®—ã€‚

* ScheduleråŸºäºèµ„æºçš„åŠ¨æ€ Batching å¼•æ“ï¼Œè´Ÿè´£ Prefill å’Œ Decode çš„ä¼˜å…ˆçº§ä»²è£ã€‚
åŠ¨æ€æ€§ï¼šå®ƒä¸æ˜¯å›ºå®š Batch Sizeï¼Œè€Œæ˜¯å®æ—¶è®¡ç®— GPU å‰©ä¸‹çš„æˆ¿é—´ï¼ˆBlocksï¼‰å¤Ÿä¸å¤Ÿã€‚
æŠ¢å æœºåˆ¶ (Preemption)ï¼šå¦‚æœæ˜¾å­˜æ»¡äº†ï¼Œå®ƒä¼šç‰ºç‰²â€œæœ€æ™šè¿›æ¥â€çš„

* Tensor Parallelismé€šè¿‡ Megatron-LM é£æ ¼çš„è¡Œåˆ—åˆ†ç‰‡ï¼Œè§£å†³äº†å•å¡æ”¾ä¸ä¸‹å¤§æ¨¡å‹æƒé‡çš„é—®é¢˜ã€‚ColumnParallelï¼šæŠŠçŸ©é˜µçºµå‘åˆ‡å¼€ï¼Œå¤§å®¶ç®—å®Œåå„æ‹¿ä¸€éƒ¨åˆ†ç»“æœï¼ˆæ— éœ€é€šä¿¡ï¼‰ã€‚
RowParallelï¼šæŠŠçŸ©é˜µæ¨ªå‘åˆ‡å¼€ï¼Œå¤§å®¶ç®—å‡ºå±€éƒ¨å’Œï¼Œæœ€åé€šè¿‡ All-Reduce æ±‡æ€»ã€‚TP çš„æ€§èƒ½ç“¶é¢ˆå¾€å¾€ä¸åœ¨è®¡ç®—ï¼Œè€Œåœ¨ All-Reduce çš„è·¨å¡å¸¦å®½ã€‚

* Attention & Triton  è¿™æ˜¯ PagedAttention çš„åº•å±‚è½åœ°ï¼Œåˆ©ç”¨ Triton è§£å†³éè¿ç»­æ˜¾å­˜çš„è¯»å†™æ•ˆç‡ã€‚å†™å…¥ (Triton)ï¼šå½“æ¨¡å‹ç®—å‡ºä¸€ä¸ªæ–°è¯æ—¶ï¼Œå®ƒä¸æ˜¯è¿ç»­å­˜çš„ã€‚Triton Kernel è´Ÿè´£æ ¹æ® slot_mapping åƒâ€œæ•£å¼¹æªâ€ä¸€æ ·ç²¾å‡†åœ°æŠŠ KV å­˜è¿›ç‰©ç†å—ã€‚
è¯»å– (FlashAttention)ï¼šåœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼Œå®ƒæ ¹æ® block_table å»ä¸åŒçš„æˆ¿é—´æ‰¾æ•°æ®ã€‚FlashAttention ç®—å­ä¿è¯äº†å³ä½¿å†…å­˜ä¸è¿ç»­ï¼Œè®¡ç®—ä¾ç„¶èƒ½ç»´æŒæé«˜çš„ç¡¬ä»¶åˆ©ç”¨ç‡ã€‚

ä¼ ç»Ÿçš„æ¨¡å‹æ¨ç†å—é™äº KV Cache å¯¹æ˜¾å­˜çš„é™æ€é¢„åˆ†é…ï¼Œå¯¼è‡´ä¸¥é‡çš„ç¢ç‰‡åŒ–ï¼Œé™åˆ¶äº†å¹¶å‘ã€‚æ‰€ä»¥å¼•å…¥äº†åˆ†é¡µç®¡ç†ï¼Œå°†æ˜¾å­˜åˆ‡å—ã€‚é…åˆåŠ¨æ€è°ƒåº¦å™¨ï¼Œæ ¹æ®å‰©ä½™å—æ•°å†³å®š Batch Sizeï¼Œå¹¶åœ¨æé™æƒ…å†µä¸‹é€šè¿‡æŠ¢å ç¡®ä¿ç³»ç»Ÿä¸å®•æœºã€‚åº•å±‚çš„è®¡ç®—ä¸èƒ½å†ç”¨åŸç”Ÿçš„ PyTorchï¼Œå¿…é¡»ä½¿ç”¨ PagedAttention æ€æƒ³çš„ç®—å­ï¼Œåˆ©ç”¨ Triton ç¼–å†™ Kernel æ¥å¤„ç†è¿™ç§éè¿ç»­çš„ KV Cache è¯»å†™ã€‚å½“æ¨¡å‹å•å¡æ”¾ä¸ä¸‹æ—¶ï¼Œå†é€šè¿‡å¼ é‡å¹¶è¡Œï¼ˆColumn/Row Parallelï¼‰å°†è´Ÿè½½å¹³æ‘Šåˆ°å¤šå¡ã€‚

## Key Features

* ğŸš€ **Fast offline inference** - Comparable inference speeds to vLLM
* ğŸ“– **Readable codebase** - Clean implementation in ~ 1,200 lines of Python code
* âš¡ **Optimization Suite** - Prefix caching, Tensor Parallelism, Torch compilation, CUDA graph, etc.

## Installation

```bash
pip install git+https://github.com/GeeeekExplorer/nano-vllm.git
```

## Model Download

To download the model weights manually, use the following command:
```bash
huggingface-cli download --resume-download Qwen/Qwen3-0.6B \
  --local-dir ~/huggingface/Qwen3-0.6B/ \
  --local-dir-use-symlinks False
```

## Quick Start

See `example.py` for usage. The API mirrors vLLM's interface with minor differences in the `LLM.generate` method:
```python
from nanovllm import LLM, SamplingParams
llm = LLM("/YOUR/MODEL/PATH", enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM."]
outputs = llm.generate(prompts, sampling_params)
outputs[0]["text"]
```

## Benchmark

See `bench.py` for benchmark.

**Test Configuration:**
- Hardware: RTX 4070 Laptop (8GB)
- Model: Qwen3-0.6B
- Total Requests: 256 sequences
- Input Length: Randomly sampled between 100â€“1024 tokens
- Output Length: Randomly sampled between 100â€“1024 tokens

**Performance Results:**
| Inference Engine | Output Tokens | Time (s) | Throughput (tokens/s) |
|----------------|-------------|----------|-----------------------|
| vLLM           | 133,966     | 98.37    | 1361.84               |
| Nano-vLLM      | 133,966     | 93.41    | 1434.13               |


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=GeeeekExplorer/nano-vllm&type=Date)](https://www.star-history.com/#GeeeekExplorer/nano-vllm&Date)